# -*- coding: utf-8 -*-
"""16027_DHIRENDRA_KUMAR_PATEL_NO_Practicals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xeFF53xSdfP3XCKFiBTvnJwbu3JzKgjJ

<H1>Numerical Optimization : Practical List (Python)
<BR>Name : DHIRENDRA KUMAR PATEL
<BR>ROLL : 16027<H1>
<h3>1. WAP for finding optimal solution using Line Search method.</h3>
"""

import numpy as np

def objective_function(x):
    return x**2 + 4*x + 4

def gradient(x):
    return 2*x + 4

def line_search(initial_x, learning_rate, epsilon):
    x = initial_x
    iteration = 0

    while True:
        gradient_x = gradient(x)
        new_x = x - learning_rate * gradient_x

        # Check for convergence
        if abs(new_x - x) < epsilon:
            break

        x = new_x
        iteration += 1

    return x, objective_function(x), iteration

# Initial parameters
initial_x = 0.0
learning_rate = 0.1
epsilon = 1e-6

result_x, result_min, iterations = line_search(initial_x, learning_rate, epsilon)

print(f"Minimum value found at x = {result_x}")
print(f"Minimum objective function value = {result_min}")
print(f"Iterations: {iterations}")

"""<h3>2. WAP to solve a LPP graphically.</h3>"""

!pip install pulp

import pulp
import matplotlib.pyplot as plt

lp_problem = pulp.LpProblem("LPP", pulp.LpMaximize)

x = pulp.LpVariable("x", lowBound=0)
y = pulp.LpVariable("y", lowBound=0)


lp_problem += 3 * x + 2 * y

lp_problem += x <= 4
lp_problem += y <= 6
lp_problem += 2 * x + y <= 12

lp_problem.solve()

print("Status:", pulp.LpStatus[lp_problem.status])

print("x =", x.varValue)
print("y =", y.varValue)

print("Optimal Value =", pulp.value(lp_problem.objective))

plt.plot(x.varValue, y.varValue, 'ro', label="Optimal Value")
plt.fill([0, 4, 4, 3, 0], [0, 0, 4, 6, 6], 'b', alpha=0.2)

plt.xlabel("x")
plt.ylabel("y")
plt.title("Graphical Solution of LPP")

plt.legend()

plt.grid(True)
plt.show()

"""<h3>3. WAP to compute the gradient and Hessian of the function <br>f(x) = 100(x<sub>2</sub> − x<sub>1</sub><sup>2</sup>)<sup>2</sup> + (1 − x1)<sup>2</sup></h3>

"""

import sympy as sp

x1,x2=sp.symbols ('x1 x2')
function= -100* (x2-x1**2) **2+(1-x1)**2

gradient=[sp.diff(function,x1), sp.diff(function, x2)]
hessian=[[sp.diff (gradient[0],x1), sp.diff (gradient[0],x2)], [sp.diff (gradient[1],x1), sp.diff (gradient [1], x2)]]

print("Gradient:", gradient)
print("\nHessian:", hessian)

"""<h3>4. WAP to find Global Optimal Solution of a function
f(x) = −10Cos(πx − 2.2) + (x + 1.5)x algebraically</h3>
"""

import numpy as np
from scipy.optimize import differential_evolution

def objective_function(x):
    return -10 * np.cos(np.pi * x - 2.2) + (x + 1.5) * x


bounds = [(-10, 10)]


result = differential_evolution(objective_function, bounds)


min_x = result.x
global_min_val = result.fun

print("global min x: ",min_x)
print("Global Optimal Solution:")
print(f"x = {min_x[0]}")
print(f"f(x) = {global_min_val}")

"""<h3>5. WAP to find Global Optimal Solution of a function
f(x) = −10Cos(πx − 2.2) + (x + 1.5)x graphically</h3>
"""

import numpy as np
import matplotlib.pyplot as plt
# Define the function f(x)
def objective_function(x):
    return -10 * np.cos(np.pi * x - 2.2) + (x + 1.5) * x
# Generate x values
x = np.linspace(-5, 5, 20)
print("X : ", x)
print()

y = objective_function(x)
print("Y : ", y)
print()

plt.plot(x, y, label='f(x) = -10Cos(pi x - 2.2) + (x + 1.5) * x')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title(' Function f(x)')
plt.grid(True)

min_y = min(y)
min_x = x[np.argmin(y)]
plt.scatter(min_x, min_y, color='blue', label=f'Minimum: ({min_x}, {min_y})')

plt.legend()
plt.show()

print("Global optimal solution is", min_x)
print("Optimal function value is", min_y)

"""<h3>6. WAP to solve constraint optimization problem.</h3>"""

from sympy import symbols, diff, solve, Matrix

x, y, l = symbols('x y lambda')
f = x**2 + y**2
g = x + y - 1

# Define the Lagrangian
L = f - l * g

# Compute partial derivatives
partials = [diff(L, var) for var in (x, y, l)]

# Solve the system of equations
solution = solve(partials, (x, y, l), dict=True)[0]

# Extract the optimal values
optimal_x = solution[x]
optimal_y = solution[y]

# Compute the Hessian matrix
# Compute the Hessian matrix using a list of lists
hessian_list = []

# Iterate over var2
for var2 in (x, y, l):
    # Initialize a row for var2
    row = []

    # Iterate over var1
    for var1 in (x, y, l):
        # Calculate the second-order partial derivative and append to the row
        row.append(diff(L.diff(var1), var2))

    # Append the row to the Hessian list
    hessian_list.append(row)

# Create an instance of the Matrix class from the list of lists
hessian_matrix = Matrix(hessian_list)

# Display the Hessian matrix
print(hessian_matrix)

hessian_determinant = hessian_matrix.det()
if hessian_determinant > 0:
    print("Stationary point is a local minimum.")
elif hessian_determinant < 0:
    print("Stationary point is a local maximum.")
else:
    print("Second-order test inconclusive (saddle point or test fails).")

# Display the result
print("Optimal solution:")
print(f"x: {optimal_x}")
print(f"y: {optimal_y}")